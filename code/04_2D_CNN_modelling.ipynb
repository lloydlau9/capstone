{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import librosa\n",
    "import librosa.display\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from matplotlib.pyplot import specgram\n",
    "import os, glob, pickle\n",
    "import IPython.display as ipd \n",
    "from tqdm import tqdm, tqdm_pandas\n",
    "import scipy\n",
    "from scipy.stats import skew\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import soundfile\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Keras\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model, model_from_json\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.callbacks import (EarlyStopping, LearningRateScheduler,\n",
    "                             ModelCheckpoint, TensorBoard, ReduceLROnPlateau)\n",
    "from keras import losses, models, optimizers\n",
    "from keras.activations import relu, softmax\n",
    "from keras.layers import (Convolution2D, GlobalAveragePooling2D, BatchNormalization, Flatten, Dropout,\n",
    "                          GlobalMaxPool2D, MaxPool2D, concatenate, Activation, Input, Dense)\n",
    "\n",
    "# sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ignore warnings \n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. Data Augmentation method   \n",
    "'''\n",
    "def speedNpitch(data):\n",
    "    \"\"\"\n",
    "    Speed and Pitch Tuning.\n",
    "    \"\"\"\n",
    "    # you can change low and high here\n",
    "    length_change = np.random.uniform(low=0.8, high = 1)\n",
    "    speed_fac = 1.2  / length_change # try changing 1.0 to 2.0 ... =D\n",
    "    tmp = np.interp(np.arange(0,len(data),speed_fac),np.arange(0,len(data)),data)\n",
    "    minlen = min(data.shape[0], tmp.shape[0])\n",
    "    data *= 0\n",
    "    data[0:minlen] = tmp[0:minlen]\n",
    "    return data\n",
    "\n",
    "'''\n",
    "2. Extracting the MFCC feature as an image (Matrix format).  \n",
    "'''\n",
    "def prepare_data(df, n, aug, mfcc):\n",
    "    X = np.empty(shape=(df.shape[0], n, 216, 1))\n",
    "    input_length = sampling_rate * audio_duration\n",
    "    \n",
    "    cnt = 0\n",
    "    for fname in tqdm(df.path):\n",
    "        file_path = fname\n",
    "        data, _ = librosa.load(file_path, sr=sampling_rate\n",
    "                               ,res_type=\"kaiser_fast\"\n",
    "                               ,duration=2.5\n",
    "                               ,offset=0.5\n",
    "                              )\n",
    "\n",
    "        # Random offset / Padding\n",
    "        if len(data) > input_length:\n",
    "            max_offset = len(data) - input_length\n",
    "            offset = np.random.randint(max_offset)\n",
    "            data = data[offset:(input_length+offset)]\n",
    "        else:\n",
    "            if input_length > len(data):\n",
    "                max_offset = input_length - len(data)\n",
    "                offset = np.random.randint(max_offset)\n",
    "            else:\n",
    "                offset = 0\n",
    "            data = np.pad(data, (offset, int(input_length) - len(data) - offset), \"constant\")\n",
    "\n",
    "        # Augmentation? \n",
    "        if aug == 1:\n",
    "            data = speedNpitch(data)\n",
    "        \n",
    "        # which feature?\n",
    "        if mfcc == 1:\n",
    "            # MFCC extraction \n",
    "            MFCC = librosa.feature.mfcc(data, sr=sampling_rate, n_mfcc=n_mfcc)\n",
    "            MFCC = np.expand_dims(MFCC, axis=-1)\n",
    "            X[cnt,] = MFCC\n",
    "            \n",
    "        else:\n",
    "            # Log-melspectogram\n",
    "            melspec = librosa.feature.melspectrogram(data, n_mels = n_melspec)   \n",
    "            logspec = librosa.amplitude_to_db(melspec)\n",
    "            logspec = np.expand_dims(logspec, axis=-1)\n",
    "            X[cnt,] = logspec\n",
    "            \n",
    "        cnt += 1\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "'''\n",
    "3. Confusion matrix plot \n",
    "'''        \n",
    "def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n",
    "    '''Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    confusion_matrix: numpy.ndarray\n",
    "        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n",
    "        Similarly constructed ndarrays can also be used.\n",
    "    class_names: list\n",
    "        An ordered list of class names, in the order they index the given confusion matrix.\n",
    "    figsize: tuple\n",
    "        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n",
    "        the second determining the vertical size. Defaults to (10,7).\n",
    "    fontsize: int\n",
    "        Font size for axes labels. Defaults to 14.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.figure.Figure\n",
    "        The resulting confusion matrix figure\n",
    "    '''\n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names, \n",
    "    )\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    \n",
    "    \n",
    "'''\n",
    "# 4. Create the 2D CNN model \n",
    "'''\n",
    "def get_2d_conv_model(n):\n",
    "    ''' Create a standard deep 2D convolutional neural network'''\n",
    "    nclass = 12\n",
    "    inp = Input(shape=(n,216,1))  #2D matrix of 30 MFCC bands by 216 audio length.\n",
    "    x = Convolution2D(32, (4,10), padding=\"same\")(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = MaxPool2D()(x)\n",
    "    x = Dropout(rate=0.2)(x)\n",
    "    \n",
    "    x = Convolution2D(32, (4,10), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = MaxPool2D()(x)\n",
    "    x = Dropout(rate=0.2)(x)\n",
    "    \n",
    "    x = Convolution2D(32, (4,10), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = MaxPool2D()(x)\n",
    "    x = Dropout(rate=0.2)(x)\n",
    "    \n",
    "    x = Convolution2D(32, (4,10), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = MaxPool2D()(x)\n",
    "    x = Dropout(rate=0.2)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64)(x)\n",
    "    x = Dropout(rate=0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Dropout(rate=0.2)(x)\n",
    "    \n",
    "    out = Dense(nclass, activation=softmax)(x)\n",
    "    model = models.Model(inputs=inp, outputs=out)\n",
    "    \n",
    "    opt = optimizers.Adam(0.001)\n",
    "    model.compile(optimizer=opt, loss=losses.categorical_crossentropy, metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "'''\n",
    "# 5. Other functions \n",
    "'''\n",
    "class get_results:\n",
    "    '''\n",
    "    We're going to create a class (blueprint template) for generating the results based on the various model approaches. \n",
    "    So instead of repeating the functions each time, we assign the results into on object with its associated variables \n",
    "    depending on each combination:\n",
    "        1) MFCC with no augmentation  \n",
    "        2) MFCC with augmentation \n",
    "        3) Logmelspec with no augmentation \n",
    "        4) Logmelspec with augmentation\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, model_history, model ,X_test, y_test, labels):\n",
    "        self.model_history = model_history\n",
    "        self.model = model\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test             \n",
    "        self.labels = labels\n",
    "\n",
    "    def create_plot(self, model_history):\n",
    "        '''Check the logloss of both train and validation, make sure they are close and have plateau'''\n",
    "        plt.plot(model_history.history['loss'])\n",
    "        plt.plot(model_history.history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()\n",
    "\n",
    "    def create_results(self, model):\n",
    "        '''predict on test set and get accuracy results'''\n",
    "        opt = optimizers.Adam(0.001)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "        score = model.evaluate(X_test, y_test, verbose=0)\n",
    "        print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n",
    "\n",
    "    def confusion_results(self, X_test, y_test, labels, model):\n",
    "        '''plot confusion matrix results'''\n",
    "        preds = model.predict(X_test, \n",
    "                                 batch_size=16, \n",
    "                                 verbose=2)\n",
    "        preds=preds.argmax(axis=1)\n",
    "        preds = preds.astype(int).flatten()\n",
    "        preds = (lb.inverse_transform((preds)))\n",
    "\n",
    "        actual = y_test.argmax(axis=1)\n",
    "        actual = actual.astype(int).flatten()\n",
    "        actual = (lb.inverse_transform((actual)))\n",
    "\n",
    "        classes = labels\n",
    "        classes.sort()    \n",
    "\n",
    "        c = confusion_matrix(actual, preds)\n",
    "        print_confusion_matrix(c, class_names = classes)\n",
    "    \n",
    "    def accuracy_results_gender(self, X_test, y_test, labels, model):\n",
    "        '''Print out the accuracy score and confusion matrix heat map of the Gender classification results'''\n",
    "    \n",
    "        preds = model.predict(X_test, \n",
    "                         batch_size=16, \n",
    "                         verbose=2)\n",
    "        preds=preds.argmax(axis=1)\n",
    "        preds = preds.astype(int).flatten()\n",
    "        preds = (lb.inverse_transform((preds)))\n",
    "\n",
    "        actual = y_test.argmax(axis=1)\n",
    "        actual = actual.astype(int).flatten()\n",
    "        actual = (lb.inverse_transform((actual)))\n",
    "        \n",
    "        # print(accuracy_score(actual, preds))\n",
    "        \n",
    "        actual = pd.DataFrame(actual).replace({'female_angry':'female'\n",
    "                   , 'female_disgust':'female'\n",
    "                   , 'female_fear':'female'\n",
    "                   , 'female_happy':'female'\n",
    "                   , 'female_sad':'female'\n",
    "                   , 'female_surprise':'female'\n",
    "                   , 'female_neutral':'female'\n",
    "                   , 'male_angry':'male'\n",
    "                   , 'male_fear':'male'\n",
    "                   , 'male_happy':'male'\n",
    "                   , 'male_sad':'male'\n",
    "                   , 'male_surprise':'male'\n",
    "                   , 'male_neutral':'male'\n",
    "                   , 'male_disgust':'male'\n",
    "                  })\n",
    "        preds = pd.DataFrame(preds).replace({'female_angry':'female'\n",
    "               , 'female_disgust':'female'\n",
    "               , 'female_fear':'female'\n",
    "               , 'female_happy':'female'\n",
    "               , 'female_sad':'female'\n",
    "               , 'female_surprise':'female'\n",
    "               , 'female_neutral':'female'\n",
    "               , 'male_angry':'male'\n",
    "               , 'male_fear':'male'\n",
    "               , 'male_happy':'male'\n",
    "               , 'male_sad':'male'\n",
    "               , 'male_surprise':'male'\n",
    "               , 'male_neutral':'male'\n",
    "               , 'male_disgust':'male'\n",
    "              })\n",
    "\n",
    "        classes = actual.loc[:,0].unique() \n",
    "        classes.sort()    \n",
    "\n",
    "        c = confusion_matrix(actual, preds)\n",
    "        print(accuracy_score(actual, preds))\n",
    "        print_confusion_matrix(c, class_names = classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>source</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male_neutral</td>\n",
       "      <td>RAVDESS</td>\n",
       "      <td>../datasets/RAVDESS/audio_speech_actors_01-24/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>male_neutral</td>\n",
       "      <td>RAVDESS</td>\n",
       "      <td>../datasets/RAVDESS/audio_speech_actors_01-24/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>male_neutral</td>\n",
       "      <td>RAVDESS</td>\n",
       "      <td>../datasets/RAVDESS/audio_speech_actors_01-24/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male_neutral</td>\n",
       "      <td>RAVDESS</td>\n",
       "      <td>../datasets/RAVDESS/audio_speech_actors_01-24/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male_neutral</td>\n",
       "      <td>RAVDESS</td>\n",
       "      <td>../datasets/RAVDESS/audio_speech_actors_01-24/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         labels   source                                               path\n",
       "0  male_neutral  RAVDESS  ../datasets/RAVDESS/audio_speech_actors_01-24/...\n",
       "1  male_neutral  RAVDESS  ../datasets/RAVDESS/audio_speech_actors_01-24/...\n",
       "2  male_neutral  RAVDESS  ../datasets/RAVDESS/audio_speech_actors_01-24/...\n",
       "3  male_neutral  RAVDESS  ../datasets/RAVDESS/audio_speech_actors_01-24/...\n",
       "4  male_neutral  RAVDESS  ../datasets/RAVDESS/audio_speech_actors_01-24/..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref = pd.read_csv(\"../datasets/data_path.csv\")\n",
    "ref.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MFCC without augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8690/8690 [04:25<00:00, 32.78it/s]\n"
     ]
    }
   ],
   "source": [
    "sampling_rate=44100\n",
    "audio_duration=2.5\n",
    "n_mfcc = 30\n",
    "mfcc = prepare_data(ref, n = n_mfcc, aug = 0, mfcc = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "408/408 [==============================] - 71s 171ms/step - loss: 2.5452 - acc: 0.1422 - val_loss: 2.1574 - val_acc: 0.2319\n",
      "Epoch 2/20\n",
      "408/408 [==============================] - 70s 172ms/step - loss: 1.9862 - acc: 0.3006 - val_loss: 1.7417 - val_acc: 0.3843\n",
      "Epoch 3/20\n",
      "125/408 [========>.....................] - ETA: 47s - loss: 1.8389 - acc: 0.3242"
     ]
    }
   ],
   "source": [
    "# Split between train and test \n",
    "X_train, X_test, y_train, y_test = train_test_split(mfcc\n",
    "                                                    , ref.labels\n",
    "                                                    , test_size=0.25\n",
    "                                                    , shuffle=True\n",
    "                                                    , random_state=42\n",
    "                                                   )\n",
    "\n",
    "\n",
    "# one hot encode the target \n",
    "lb = LabelEncoder()\n",
    "y_train = np_utils.to_categorical(lb.fit_transform(y_train))\n",
    "y_test = np_utils.to_categorical(lb.fit_transform(y_test))\n",
    "\n",
    "# Normalization as per the standard NN process\n",
    "mean = np.mean(X_train, axis=0)\n",
    "std = np.std(X_train, axis=0)\n",
    "\n",
    "X_train = (X_train - mean)/std\n",
    "X_test = (X_test - mean)/std\n",
    "\n",
    "# Build CNN model \n",
    "model = get_2d_conv_model(n=n_mfcc)\n",
    "model_history = model.fit(X_train, y_train, validation_data=(X_test, y_test), \n",
    "                    batch_size=16, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_results(model_history,model,X_test,y_test, ref.labels.unique())\n",
    "results.create_plot(model_history)\n",
    "results.create_results(model)\n",
    "results.confusion_results(X_test, y_test, ref.labels.unique(), model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MFCC with Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate=44100\n",
    "audio_duration=2.5\n",
    "n_mfcc = 30\n",
    "mfcc_aug = prepare_data(ref, n = n_mfcc, aug = 1, mfcc = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split between train and test \n",
    "X_train, X_test, y_train, y_test = train_test_split(mfcc_aug\n",
    "                                                    , ref.labels\n",
    "                                                    , test_size=0.25\n",
    "                                                    , shuffle=True\n",
    "                                                    , random_state=42\n",
    "                                                   )\n",
    "\n",
    "# one hot encode the target \n",
    "lb = LabelEncoder()\n",
    "y_train = np_utils.to_categorical(lb.fit_transform(y_train))\n",
    "y_test = np_utils.to_categorical(lb.fit_transform(y_test))\n",
    "\n",
    "# Normalization as per the standard NN process\n",
    "# mean = np.mean(X_train, axis=0)\n",
    "# std = np.std(X_train, axis=0)\n",
    "\n",
    "# X_train = (X_train - mean)/std\n",
    "# X_test = (X_test - mean)/std\n",
    "\n",
    "# Build CNN model \n",
    "model = get_2d_conv_model(n=n_mfcc)\n",
    "model_history = model.fit(X_train, y_train, validation_data=(X_test, y_test), \n",
    "                    batch_size=16, verbose = 2, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_results(model_history,model,X_test,y_test, ref.labels.unique())\n",
    "results.create_plot(model_history)\n",
    "results.create_results(model)\n",
    "results.confusion_results(X_test, y_test, ref.labels.unique(), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
